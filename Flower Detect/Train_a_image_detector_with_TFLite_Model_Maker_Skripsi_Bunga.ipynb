{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Train a image detector with TFLite Model Maker Skripsi Bunga",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhorihasan/MachineLearning/blob/main/Train_a_image_detector_with_TFLite_Model_Maker_Skripsi_Bunga.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Train a Flower detector with TensorFlow Lite Model Maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## Prerequisites\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvAObmTqglq"
      },
      "source": [
        "### Install the required packages\n",
        "Start by installing the required packages, including the Model Maker package  and the pycocotools library you'll use for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhl8lqVamEty"
      },
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q pycocotools\n",
        "!pip install -q tflite-support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6lRhVK9Q_0U"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5M8iuydhVae"
      },
      "source": [
        "## Train your salad detection model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xushUyZXqP59"
      },
      "source": [
        "There are six steps to training an object detection model:\n",
        "\n",
        "**Step 1. Choose an object detection model archiecture.**\n",
        "\n",
        "This tutorial uses the EfficientDet-Lite2 model. EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture. \n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 260           | 41.96%               |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtdZ-JDwMimd"
      },
      "source": [
        "spec = model_spec.get('efficientdet_lite2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz_f-yPPxEP0",
        "outputId": "4f9e1c6f-7464-4c47-e02e-b0610dfed6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5U-A3tw6Y27"
      },
      "source": [
        "**Step 2. Load the dataset.**\n",
        "\n",
        "Model Maker will take input data in the xml format. Use the `ObjectDetectorDataloader.from_pascal_voc` method to load the dataset and split them into the training, validation and test images.\n",
        "\n",
        "* Training images: These images are used to train the object detection model to recognize flower.\n",
        "* Validation images: These are images that the model didn't see during the training process. You'll use them to decide when you should stop the training, to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
        "* Test images: These images are used to evaluate the final model performance.\n",
        "\n",
        "You can load the xml file directly from Google Cloud Storage, but you don't need to keep your images on Google Cloud to use Model Maker. You can specify a local xml file on your computer, and Model Maker will work just fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7wwyI2gLKns"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc('/content/drive/MyDrive/dataset/train', '/content/drive/MyDrive/dataset/train', ['daisy', 'dandelion', 'rose', 'sunflower','tulip'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_nGbWyKMANB"
      },
      "outputs": [],
      "source": [
        "validation_data = object_detector.DataLoader.from_pascal_voc('/content/drive/MyDrive/dataset/test', '/content/drive/MyDrive/dataset/test', ['daisy', 'dandelion', 'rose', 'sunflower','tulip'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uZkLR6N6gDR"
      },
      "source": [
        "**Step 3. Train the TensorFlow model with the training data.**\n",
        "\n",
        "* The EfficientDet-Lite0 model uses `epochs = 50` by default, which means it will go through the training dataset 50 times. You can look at the validation accuracy during training and stop early to avoid overfitting.\n",
        "* Set `batch_size = 8` here so you will see that it takes 109 steps to go through the 750 images in the training dataset. \n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwlYdTcg63xy"
      },
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BzCHLWJ6h7q"
      },
      "source": [
        "**Step 4. Evaluate the model with the test data.**\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the remaining 250 images in the test dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 1 step to go through the 250 images in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xmnl6Yy7ARn"
      },
      "source": [
        "model.evaluate(validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCDMe0e6jlT"
      },
      "source": [
        "**Step 5.  Export as a TensorFlow Lite model.**\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is full integer quantization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm_UULdW7A9T"
      },
      "source": [
        "model.export(export_dir='.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UygYErfCD5m3"
      },
      "source": [
        "**Step 6.  Evaluate the TensorFlow Lite model.**\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop. \n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHYDWcljr6jq"
      },
      "source": [
        "model.evaluate_tflite('model.tflite', validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVxaf3x_7OfB"
      },
      "source": [
        "You can download the TensorFlow Lite model file using the left sidebar of Colab. Right-click the `model.tflite` file and choose `Download` to download it to your local computer.\n",
        "\n",
        "In the next step of the codelab, you'll use the [ObjectDetector API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector) of the [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview) to integrate the model into the Android app."
      ]
    }
  ]
}